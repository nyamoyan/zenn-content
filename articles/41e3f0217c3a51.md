---
title: "ラベルなしデータで自己改善する強化学習手法-TTRL"
emoji: "🕌"
type: "tech" # tech: 技術記事 / idea: アイデア
topics: [Tech, LLM]
published: true
---

# 目的

[alphaXiv](https://www.alphaxiv.org)を見てたら，目に留まったので，自分なりにまとめてみる．

# どんな論文か

LLM の推論タスクに関して，理想的なラベルがないデータで強化学習を行う手法（Test-Time Reinforcement Learning; TTRL）を提案．

TTRL は、以下の 4 つのベンチマークで評価され，複雑な推論タスクの性能を大幅に向上させた．
（詳細な改善度合いについては論文を確認）

- GPQA-Diamond (2024) によって提案された、大学院レベルの Google-Proof
- Question Answering ベンチマークの、挑戦的で質の高いサブセット
- AIME 2024: (2024) による数学的推論ベンチマーク
- AMC: (2024) による数学的推論ベンチマーク
- MATH-500: (2021) による数学的推論ベンチマーク

https://arxiv.org/abs/2504.16084

https://github.com/PRIME-RL/TTRL

# 背景・課題

- LLM に対する従来の強化学習アプローチは，人間のアノテーションデータまたは検証可能な正解に大きく依存している．これは複雑な推論タスクでは非常に費用がかかり，非現実的．

# TTRL フレームワーク

TTRL フレームワークは，次の 4 つのステップからなる．

1. 多数決によるラベル推定
2. ルールベースの報酬計算
3. 方策最適化
4. 反復的な自己改善

![](/images/41e3f0217c3a51/image.png)

## 1. 多数決によるラベル推定

各テスト問題に対して，モデルは繰り返しサンプリングを行い，複数の候補応答を生成.
これらの応答は回答抽出器を通して予測回答を得た後，多数決によって最終的な合意回答（擬似ラベル）を決定.

## 2. ルールベースの報酬計算

多数決で得られた擬似ラベルを用いて，各個別応答に対して二値の報酬関数を計算．

$$
R(\hat{y}^i, y^*) =
\begin{cases}
1, & \text{if } \hat{y}^i = y^* \\
0, & \text{otherwise}
\end{cases}
$$

## 3. 方策最適化

強化学習を用いて，期待報酬を最大化するようにモデルパラメータを更新．

$$
\theta \leftarrow \theta + \eta \nabla_{\theta} \mathbb{E}_{y \sim \pi_{\theta}(\cdot \mid x)} \left[ r(y, y^*) \right]
$$

## 4. 反復的な自己改善

このプロセスを繰り返すことで，モデル性能の向上がより良い擬似ラベルを生み出し，それがさらに性能向上へとつながる**正のフィードバックループ**が形成される．

# 感想

LLM の学習は

- 事前学習（Pre-training）
- 微調整（Fine-tuning）
- RLHF（Reinforcement Learning with Human Feedback）

の 3 段階から構成されるが，

この中で TTRL は、ファインチューニング段階で，特に強化学習の枠組みを「ラベルなしのテストデータ」という新たな設定に適用しようとした研究かな．

最初に，「半教師あり学習の一種である Pseudo-Label 手法じゃん！」ってなった．
「モデルが自分で推定したラベルを使って自己改善する」という発想は共通しているしね．

でも，複雑な推論タスクで，性能大幅に改善しているから，まさに，人間の「教え」なしに自律的に性能を向上させる AI への一歩としての興味深いアプローチと思った．
